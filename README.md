# Origo - Breaking down the signals of writing origin

Origo is a comprehensive web application that analyzes English text to estimate the probability that it was generated by artificial intelligence. The application provides explainable AI detection through multiple analysis heuristics with detailed breakdowns at word, sentence and paragraph levels.

## âš ï¸ Ethical Disclaimer

**Important:** AI-generated text detection is imperfect. This tool provides probabilistic signals â€” not conclusive proof. It is meant to support human judgment, not replace it.

## ğŸ¯ Features

- **Multi-heuristic Analysis**: Combines perplexity, burstiness, semantic coherence, and n-gram similarity
- **Granular Insights**: Analysis at paragraph, sentence, and word levels
- **Interactive Visualization**: Color-coded highlights and tooltips
- **Responsive Design**: Works on desktop and tablet devices
- **Real-time Processing**: Fast analysis with progress indication
- **Word Impact Analysis**: Detailed breakdown of influential words

## ğŸ›  Technology Stack

### Backend
- **Framework**: FastAPI (Python)
- **AI/ML**: Transformers, PyTorch, Sentence-BERT
- **NLP**: NLTK, spaCy, scikit-learn
- **Models**: GPT-2 (perplexity), all-MiniLM-L6-v2 (semantic analysis)

### Frontend
- **Framework**: React 18 with TypeScript
- **Build Tool**: Vite
- **UI**: Custom CSS with responsive design
- **HTTP Client**: Axios
- **Icons**: Lucide React

### Deployment
- **Containerization**: Docker & Docker Compose
- **Web Server**: Nginx (frontend proxy)
- **Cross-platform**: Windows, macOS, Linux support

## ğŸ“‹ Prerequisites

- **Node.js** 18+ (for frontend development)
- **Python** 3.11+ (for backend development)
- **Docker & Docker Compose** (for deployment)
- **Git** (for version control)

## ğŸš€ Quick Start with Docker

The fastest way to run Origo is using Docker Compose:

```bash
# Clone the repository
git clone <repository-url>
cd origo

# Start the application
docker-compose up --build

# Access the application
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
# API Documentation: http://localhost:8000/docs
```

## ğŸ’» Development Setup

### Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"

# Start development server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Setup

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev

# Build for production
npm run build
```

## ğŸ“– API Documentation

### Analyze Text Endpoint

**POST** `/analyze`

Analyzes text for AI-generated content detection.

#### Request Body
```json
{
  "text": "Your text to analyze here..."
}
```

#### Response
```json
{
  "overall_score": 0.78,
  "global_scores": {
    "perplexity": 0.83,
    "burstiness": 0.72,
    "semantic_coherence": 0.65,
    "ngram_similarity": 0.89
  },
  "paragraphs": [
    {
      "text": "Paragraph text...",
      "score": 0.80,
      "sentences": [
        {
          "text": "Sentence text...",
          "score": 0.91,
          "words": [
            {"word": "thus", "score": 0.92}
          ]
        }
      ]
    }
  ],
  "word_analysis": {
    "unique_words": [
      {"word": "hence", "average_score": 0.91, "count": 3}
    ]
  }
}
```

### Other Endpoints

- **GET** `/` - API information
- **GET** `/health` - Health check
- **GET** `/docs` - Interactive API documentation

## ğŸ”§ Backend Architecture & Classes

### Analysis Module Classes

#### 1. PerplexityAnalyzer (`backend/analysis/perplexity.py`)
**Status**: âœ… **Enhanced & Production Ready** (Updated with Advanced Detection)

**Purpose**: Multi-dimensional perplexity analysis combining GPT-2 language model scoring with stylistic pattern detection and register authenticity analysis.

**Enhanced Components**:
- **Base Perplexity** (40%): Core GPT-2 perplexity calculation
- **Stylistic Patterns** (30%): AI vocabulary and phrasal pattern detection
- **Register Authenticity** (30%): Formality consistency and naturalness analysis

**Key Methods**:
- `analyze_text(text: str) -> Dict`: **NEW** - Comprehensive analysis combining all components
- `calculate_perplexity(text: str) -> float`: Core perplexity calculation with normalization
- `analyze_stylistic_patterns(text: str) -> Dict`: **NEW** - Detects AI-specific vocabulary and phrases
- `analyze_register_authenticity(text: str) -> Dict`: **NEW** - Analyzes formality and naturalness markers
- `analyze_sentences(sentences: List[str]) -> List[Dict]`: Sentence-level analysis with word breakdown
- `analyze_paragraphs(text: str) -> List[Dict]`: Paragraph-level analysis with sentence breakdown
- `get_word_impact_analysis(text: str) -> Dict`: Word-level impact analysis for highlighting

**Stylistic Pattern Detection**:
- **Suspicious Vocabulary**: Detects AI-preferred verbs ("delve", "leverage", "underscore")
- **Formulaic Phrases**: Identifies mechanical transitions ("it's worth noting that", "in essence")
- **Punctuation Patterns**: Analyzes Oxford comma overuse, em dash abuse, mechanical formatting
- **Transition Constructs**: Detects artificial structuring ("firstly...secondly...finally")

**Register Authenticity Analysis**:
- **Formality Inconsistencies**: Detects mixed casual/academic registers
- **Emotional Variance**: Analyzes unnatural emotional consistency
- **Naturalness Patterns**: Identifies artificial vs. natural language constructions
- **Discourse Markers**: Detects mechanical vs. natural transitions

**Configuration Files**:
- `data/perplexity/llm_red_flags.json`: Suspicious vocabulary patterns
- `data/perplexity/punctuation_patterns.json`: Punctuation analysis rules
- `data/perplexity/register_authenticity.json`: Formality and naturalness markers

**AI Detection Logic**: Enhanced multi-dimensional scoring where base perplexity, stylistic patterns, and register authenticity are weighted together. Higher combined scores indicate stronger AI probability.

**Development Notes**: 
- âœ… **Enhanced with advanced pattern recognition**
- âœ… **Configuration-driven detection rules**
- âœ… **Weighted component scoring system**
- âœ… **Comprehensive linguistic analysis**
- Uses transformer models for high accuracy
- Implements proper tokenization and device management
- Includes comprehensive error handling
- Memory efficient with model caching

#### 2. BurstinessAnalyzer (`backend/analysis/burstiness.py`)
**Status**: âœ… **Enhanced & Production Ready** (Updated with Structural Analysis)

**Purpose**: Multi-dimensional burstiness analysis combining sentence variation patterns with structural consistency detection.

**Enhanced Components**:
- **Base Burstiness** (60%): Traditional sentence length and complexity variation
- **Structural Consistency** (40%): Detects artificial document structure patterns

**Key Methods**:
- `analyze_text(text: str) -> Dict`: **ENHANCED** - Comprehensive analysis combining base and structural components
- `calculate_sentence_length_variation(sentences: List[str]) -> float`: Analyzes sentence length diversity
- `calculate_syntactic_complexity_variation(sentences: List[str]) -> float`: Examines structural complexity patterns
- `calculate_sentence_start_variation(sentences: List[str]) -> float`: Detects repetitive sentence openings
- `calculate_punctuation_patterns(text: str) -> float`: Analyzes punctuation usage diversity
- `analyze_structural_consistency(text: str) -> Dict`: **NEW** - Detects artificial document structure

**Structural Consistency Detection**:
- **Paragraph Uniformity**: Detects unnaturally uniform paragraph lengths
- **Sentence Symmetry**: Identifies mechanical sentence structure patterns
- **Section Balance**: Analyzes artificial intro/body/conclusion ratios
- **Mechanical Patterns**: Detects overuse of transition phrases and formulaic structures
- **Rhythmic Patterns**: Identifies artificially consistent sentence rhythm
- **Formatting Consistency**: Detects unnaturally perfect punctuation and formatting

**Configuration Files**:
- `data/burstiness/structural_patterns.json`: Structural uniformity detection rules

**AI Detection Logic**: Enhanced scoring where base burstiness metrics are combined with structural consistency analysis. Uniform patterns in both dimensions indicate higher AI probability.

**Development Notes**:
- âœ… **Enhanced with structural pattern detection**
- âœ… **Configuration-driven uniformity analysis**
- âœ… **Weighted component scoring system**
- âœ… **Advanced linguistic metrics**
- Implements multiple linguistic metrics
- Weighted scoring system for different pattern types
- Statistical analysis of text variation
- No external dependencies required

#### 3. NgramAnalyzer (`backend/analysis/ngram.py`) 
**Status**: âœ… **Fully Implemented & Production Ready**

**Purpose**: Detects repetitive patterns and n-gram similarities common in AI-generated text.

**Key Methods**:
- `extract_ngrams(text: str, n: int) -> List[Tuple]`: Extracts n-gram sequences
- `calculate_ngram_repetition(text: str, n: int) -> float`: Measures n-gram repetition rates
- `calculate_phrase_repetition(text: str) -> float`: Detects repetitive phrases
- `calculate_transition_predictability(text: str) -> float`: Analyzes word transition patterns
- `calculate_lexical_diversity(text: str) -> float`: Measures vocabulary diversity
- `analyze_text(text: str) -> Dict`: Comprehensive n-gram analysis

**AI Detection Logic**: High repetition rates and predictable transitions indicate AI generation. Uses entropy calculations and frequency analysis.

**Development Notes**:
- Implements multiple n-gram sizes (2-5)
- Uses Counter and statistical methods
- Entropy-based predictability scoring
- Moving window analysis for consistency

#### 4. SemanticAnalyzer (`backend/analysis/semantic.py`)
**Status**: âœ… **Fully Implemented & Production Ready**

**Purpose**: Uses Sentence-BERT embeddings to analyze semantic coherence and flow patterns.

**Key Methods**:
- `get_sentence_embeddings(sentences: List[str]) -> np.ndarray`: Generates sentence embeddings
- `calculate_semantic_coherence(sentences: List[str]) -> float`: Measures overall semantic consistency
- `calculate_semantic_flow(sentences: List[str]) -> float`: Analyzes flow between consecutive sentences
- `calculate_topic_consistency(text: str) -> float`: Measures topic consistency across paragraphs
- `calculate_semantic_repetition(sentences: List[str]) -> float`: Detects semantic redundancy
- `analyze_text(text: str) -> Dict`: Comprehensive semantic analysis

**AI Detection Logic**: Extreme semantic coherence or unusual flow patterns may indicate AI generation. Uses cosine similarity between embeddings.

**Development Notes**:
- Uses Sentence-BERT (all-MiniLM-L6-v2) model
- Implements similarity matrix calculations
- Sklearn integration for similarity metrics
- Sophisticated pattern recognition algorithms

#### 5. ScoreFusion (`backend/analysis/scoring.py`)
**Status**: âœ… **Enhanced & Production Ready** (Updated Scoring Algorithm)

**Purpose**: Combines multiple analysis heuristics using configurable arithmetic mean weighting for optimal AI detection accuracy.

**Enhanced Features**:
- **Configuration-Driven Weights**: Loads scoring weights from `data/weights_config.json`
- **Simple Arithmetic Mean**: Equal weighting approach (25% each component)
- **Enhanced Analyzer Integration**: Works with extended perplexity and burstiness analyzers
- **Feature Flag Support**: Configurable enable/disable of enhanced features

**Key Methods**:
- `analyze_text_comprehensive(text: str) -> Dict`: **ENHANCED** - Main analysis orchestrator with extended results
- `validate_weights()`: Ensures scoring weights sum to 1.0
- `_analyze_paragraphs(text: str) -> List[Dict]`: **ENHANCED** - Uses extended analyzers
- `_analyze_sentences(sentences: List[str]) -> List[Dict]`: **ENHANCED** - Uses extended analyzers
- `_analyze_words(text: str) -> Dict`: Word-level impact analysis
- `_extract_score(result: Any, analysis_type: str) -> float`: **NEW** - Handles both enhanced and legacy formats

**Scoring Weights** (Enhanced Configuration):
- **Perplexity**: 25% (equal weight with enhanced sub-components)
- **Burstiness**: 25% (equal weight with enhanced sub-components)
- **N-gram Similarity**: 25% (equal weight)
- **Semantic Coherence**: 25% (equal weight)

**Enhanced Response Structure**:
```json
{
  "overall_score": 0.78,
  "global_scores": { /* ... */ },
  "enhanced_analysis": {
    "perplexity_details": {
      "overall_score": 0.83,
      "base_perplexity": 0.80,
      "stylistic_patterns": { /* detailed breakdown */ },
      "register_authenticity": { /* detailed breakdown */ }
    },
    "burstiness_details": {
      "overall_score": 0.72,
      "base_burstiness": 0.70,
      "structural_consistency": { /* detailed breakdown */ }
    }
  },
  "analysis_metadata": {
    "enhanced_features_enabled": {
      "stylistic_analysis": true,
      "register_analysis": true,
      "structural_analysis": true
    }
  }
}
```

**Configuration Files**:
- `data/weights_config.json`: Scoring weights and feature flags

**Development Notes**:
- âœ… **Enhanced with configuration-driven scoring**
- âœ… **Supports both enhanced and legacy analyzer formats**
- âœ… **Feature flag system for controlled rollout**
- âœ… **Detailed enhanced analysis reporting**
- Implements weighted fusion algorithm
- Comprehensive error handling for each analyzer
- Hierarchical analysis (text â†’ paragraph â†’ sentence â†’ word)
- Production-ready with extensive logging

### Utility Classes

#### ModelLoader (`backend/utils/model_loader.py`)
**Status**: âœ… **Production Ready Singleton**

**Purpose**: Efficiently manages AI model loading with singleton pattern to prevent memory waste.

**Key Methods**:
- `get_gpt2_model() -> tuple`: Loads GPT-2 model and tokenizer
- `get_sentence_transformer() -> SentenceTransformer`: Loads Sentence-BERT model
- `get_device() -> str`: Determines optimal device (CPU/GPU)
- `preload_all_models()`: Startup optimization method

**Models Managed**:
- **GPT-2**: For perplexity calculations
- **all-MiniLM-L6-v2**: For semantic analysis
- **Device Management**: Automatic CPU/GPU detection

#### SentenceSplitter (`backend/utils/sentence_splitter.py`)
**Status**: âœ… **Production Ready**

**Purpose**: Handles text preprocessing, tokenization, and segmentation with NLTK integration.

**Key Methods**:
- `clean_text(text: str) -> str`: Text normalization and cleaning
- `split_into_sentences(text: str) -> List[str]`: NLTK-based sentence segmentation
- `split_into_paragraphs(text: str) -> List[str]`: Paragraph extraction
- `tokenize_words(text: str, remove_stopwords: bool) -> List[str]`: Word tokenization
- `extract_word_positions(text: str, target_words: List[str]) -> Dict`: Position mapping for UI highlighting
- `get_text_statistics(text: str) -> Dict`: Text metrics calculation

### API Integration (`backend/main.py`)
**Status**: âœ… **Production Ready FastAPI Application**

**Key Features**:
- FastAPI framework with automatic OpenAPI documentation
- CORS middleware for frontend integration
- Health check endpoints
- Comprehensive error handling
- Request/response validation with Pydantic models
- Model preloading for optimal performance

## ğŸ“Š Backend Output Structure

### Main Analysis Response

The `/analyze` endpoint returns a comprehensive JSON structure:

```json
{
  "overall_score": 0.78,                    // Combined AI probability (0-1)
  "global_scores": {                        // Individual component scores
    "perplexity": 0.83,                     // GPT-2 perplexity analysis
    "burstiness": 0.72,                     // Sentence pattern variation
    "semantic_coherence": 0.65,             // Semantic flow consistency
    "ngram_similarity": 0.89                // N-gram repetition patterns
  },
  "paragraphs": [                           // Paragraph-level analysis
    {
      "text": "Paragraph content...",
      "score": 0.80,                        // Paragraph AI probability
      "sentences": [                        // Nested sentence analysis
        {
          "text": "Sentence content...",
          "score": 0.91,                    // Sentence AI probability
          "words": [                        // Word-level contributions
            {
              "word": "thus",
              "score": 0.92                 // Word AI probability
            }
          ]
        }
      ]
    }
  ],
  "word_analysis": {                        // Global word impact analysis
    "unique_words": [
      {
        "word": "hence",
        "average_score": 0.91,               // Average AI probability for word
        "count": 3                          // Frequency in text
      }
    ]
  },
  "analysis_metadata": {                    // Text statistics and analysis info
    "text_length": 1247,                    // Character count
    "word_count": 189,                      // Word count
    "sentence_count": 12,                   // Sentence count
    "paragraph_count": 3,                   // Paragraph count
    "weights_used": {                       // Scoring weights applied
      "perplexity": 0.4,
      "burstiness": 0.2,
      "semantic_coherence": 0.2,
      "ngram_similarity": 0.2
    }
  }
}
```

### Score Interpretation

**Score Ranges**:
- **0.0 - 0.4**: Low AI probability (likely human-written) ğŸŸ¢
- **0.4 - 0.7**: Medium AI probability (uncertain) ğŸŸ¡  
- **0.7 - 1.0**: High AI probability (likely AI-generated) ğŸ”´

**Component Score Details**:

1. **Perplexity Score** (0-1): 
   - Measures text predictability using GPT-2
   - Higher = more predictable = more AI-like
   - Most reliable single indicator (40% weight)

2. **Burstiness Score** (0-1):
   - Measures sentence structure variation
   - Higher = more uniform = more AI-like
   - Analyzes length, complexity, punctuation patterns

3. **N-gram Similarity Score** (0-1):
   - Measures repetitive patterns and phrases
   - Higher = more repetitive = more AI-like
   - Includes bigram, trigram, and phrase analysis

4. **Semantic Coherence Score** (0-1):
   - Measures semantic flow and consistency
   - Extreme values (very high/low) = more AI-like
   - Uses Sentence-BERT embeddings for analysis

### Hierarchical Analysis Structure

The system provides analysis at multiple granularities:

1. **Document Level**: Overall AI probability and global component scores
2. **Paragraph Level**: Individual paragraph scores with sentence breakdown
3. **Sentence Level**: Individual sentence scores with word contributions
4. **Word Level**: Word-specific AI probability contributions and frequencies

This hierarchical approach enables precise identification of AI-generated content at different text levels, supporting detailed user interface highlighting and explanations.

## ğŸ“‹ Enhanced Configuration System

### Data-Driven Analysis Configuration

Origo now uses a sophisticated configuration system that allows fine-tuning of AI detection parameters without code changes:

**Configuration Files Structure**:
```
backend/data/
â”œâ”€â”€ weights_config.json          # Global scoring weights and feature flags
â”œâ”€â”€ perplexity/
â”‚   â”œâ”€â”€ llm_red_flags.json      # AI-specific vocabulary patterns
â”‚   â”œâ”€â”€ punctuation_patterns.json # Suspicious punctuation analysis
â”‚   â””â”€â”€ register_authenticity.json # Formality and naturalness markers
â””â”€â”€ burstiness/
    â””â”€â”€ structural_patterns.json # Document structure uniformity detection
```

### Key Configuration Features

#### 1. Scoring Weight Configuration (`weights_config.json`)
```json
{
  "scoring_weights": {
    "perplexity": 0.25,        // Equal arithmetic mean weighting
    "burstiness": 0.25,        // As specified in MODIFICHE.md
    "semantic_coherence": 0.25,
    "ngram_similarity": 0.25
  },
  "perplexity_components": {
    "base_perplexity": 0.4,    // Component weighting within perplexity
    "stylistic_patterns": 0.3,
    "register_authenticity": 0.3
  },
  "burstiness_components": {
    "base_burstiness": 0.6,     // Component weighting within burstiness
    "structural_consistency": 0.4
  },
  "feature_flags": {
    "enable_enhanced_perplexity": true,
    "enable_enhanced_burstiness": true,
    "enable_stylistic_analysis": true,
    "enable_register_analysis": true,
    "enable_structural_analysis": true
  }
}
```

#### 2. AI Vocabulary Detection (`llm_red_flags.json`)
- **Suspicious Verbs**: "delve", "leverage", "underscore", "foster", "transcend"
- **Suspicious Modifiers**: "thought-provoking", "nuanced", "multifaceted", "seamless"
- **Formulaic Phrases**: "it's worth noting that", "in essence", "at the end of the day"
- **Transition Constructs**: "firstly...secondly...finally" patterns
- **Weighted Scoring**: Different weights for different pattern types

#### 3. Structural Pattern Detection (`structural_patterns.json`)
- **Paragraph Uniformity**: Variance thresholds for suspicious consistency
- **Sentence Symmetry**: Repetition thresholds for mechanical patterns
- **Mechanical Transitions**: Overuse detection for formulaic phrases
- **Rhythmic Analysis**: Syllable variance thresholds for artificial rhythm

#### 4. Register Authenticity Analysis (`register_authenticity.json`)
- **Formality Mixing**: Detection of casual/academic register inconsistencies
- **Emotional Variance**: Thresholds for unnatural emotional consistency
- **Naturalness Markers**: Artificial vs. natural language construction patterns
- **Discourse Analysis**: Mechanical vs. natural transition patterns

### Configuration Benefits

âœ… **Flexible Tuning**: Adjust detection sensitivity without code changes
âœ… **Rapid Deployment**: Update detection rules via configuration files
âœ… **A/B Testing**: Feature flags enable controlled rollout of enhancements
âœ… **Domain Adaptation**: Customize detection for specific text types
âœ… **Transparent Rules**: Human-readable detection criteria
âœ… **Version Control**: Track detection rule changes over time

## ğŸ”§ Enhanced Analysis Components

### 1. Enhanced Perplexity Analysis
- **Base Analysis**: Uses GPT-2 language model for predictability measurement
- **Stylistic Patterns**: Detects AI-specific vocabulary and formulaic phrases
- **Register Authenticity**: Analyzes formality consistency and naturalness markers
- **Weight**: 25% (equal weighting as per MODIFICHE.md)

### 2. Enhanced Burstiness Analysis
- **Base Analysis**: Analyzes sentence length and complexity variation
- **Structural Consistency**: Detects artificial document structure patterns
- **Mechanical Detection**: Identifies formulaic transitions and formatting
- **Weight**: 25% (equal weighting)

### 3. Semantic Coherence Analysis
- Uses Sentence-BERT embeddings for semantic flow analysis
- Measures semantic consistency and topic coherence
- Detects extreme coherence patterns suggesting AI generation
- **Weight**: 25% (equal weighting)

### 4. N-gram Similarity Analysis
- Detects repetitive patterns and phrase reuse
- Analyzes n-gram frequencies and lexical diversity
- Identifies mechanical language patterns common in AI text
- **Weight**: 25% (equal weighting)

### Enhanced Detection Capabilities

**Multi-Dimensional Analysis**:
- âœ… **Vocabulary Analysis**: AI-specific word usage patterns
- âœ… **Syntactic Analysis**: Mechanical sentence structure detection
- âœ… **Semantic Analysis**: Flow and coherence pattern recognition
- âœ… **Stylistic Analysis**: Register and formality consistency
- âœ… **Structural Analysis**: Document organization pattern detection
- âœ… **Rhythmic Analysis**: Artificial rhythm and pacing detection

**Configuration-Driven Detection**:
- âœ… **Customizable Thresholds**: Adjust sensitivity per use case
- âœ… **Feature Toggles**: Enable/disable specific detection methods
- âœ… **Weight Adjustment**: Fine-tune component importance
- âœ… **Pattern Updates**: Update detection rules without code changes

## ğŸ¨ User Interface

### Main Features
- **Text Input**: Large textarea with character/word count
- **Analysis Results**: Tabbed interface with multiple views
- **Score Visualization**: Circular progress indicators
- **Highlighted Text**: Color-coded analysis with tooltips
- **Word Table**: Sortable table with word-level insights

### Color Coding
- ğŸŸ¢ **Green**: Low AI probability (0-60%)
- ğŸŸ¡ **Yellow**: Medium AI probability (60-70%)
- ğŸ”´ **Red**: High AI probability (70-100%)

## ğŸ³ Docker Deployment

### Production Deployment

```bash
# Build and start services
docker-compose up -d --build

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Update and restart
docker-compose pull
docker-compose up -d --build
```

### Environment Variables

Create `.env` file for custom configuration:

```env
# Backend
PYTHONPATH=/app
PYTHONUNBUFFERED=1

# Frontend
VITE_API_URL=http://localhost:8000
```

## ğŸ§ª Testing

### Backend Testing
```bash
cd backend
pytest tests/ -v
```

### Frontend Testing
```bash
cd frontend
npm test
```

### Integration Testing
```bash
# Start services
docker-compose up -d

# Test API endpoints
curl http://localhost:8000/health
curl -X POST http://localhost:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{"text": "This is a test sentence."}'

# Run automated integration tests
# Windows:
cd tests
.\integration-test.ps1

# Linux/macOS:
cd tests
./integration-test.sh
```

## ğŸ“Š Performance Considerations

- **Model Loading**: Models are loaded once at startup
- **Request Timeout**: 2-minute timeout for analysis
- **Text Limits**: 10-50,000 character range
- **Memory Usage**: ~2GB RAM recommended for models
- **Processing Time**: 5-30 seconds depending on text length

## ğŸš€ Deployment Options

### Local Development
- Backend: `uvicorn main:app --reload`
- Frontend: `npm run dev`

### Docker Compose (Recommended)
- Full-stack deployment with one command
- Automatic service orchestration
- Health checks and restart policies

### Cloud Deployment
- **Backend**: Deploy to Heroku, Railway, or AWS
- **Frontend**: Deploy to Vercel, Netlify, or AWS S3
- **Full-stack**: Use Docker on DigitalOcean, AWS ECS, or Google Cloud Run

## ğŸ“ Project Structure

```
origo/
â”œâ”€â”€ backend/              # Python FastAPI application
â”‚   â”œâ”€â”€ analysis/         # AI detection modules
â”‚   â”œâ”€â”€ utils/           # Utilities and model management
â”‚   â”œâ”€â”€ data/            # ğŸ†• Configuration files for enhanced detection
â”‚   â”‚   â”œâ”€â”€ perplexity/  # Stylistic and register analysis configs
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_red_flags.json
â”‚   â”‚   â”‚   â”œâ”€â”€ punctuation_patterns.json
â”‚   â”‚   â”‚   â””â”€â”€ register_authenticity.json
â”‚   â”‚   â”œâ”€â”€ burstiness/  # Structural consistency configs
â”‚   â”‚   â”‚   â””â”€â”€ structural_patterns.json
â”‚   â”‚   â””â”€â”€ weights_config.json  # Global scoring configuration
â”‚   â”œâ”€â”€ main.py          # FastAPI app entry point
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ frontend/            # React TypeScript application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # React components
â”‚   â”‚   â”œâ”€â”€ services/    # API communication
â”‚   â”‚   â””â”€â”€ types/       # TypeScript definitions
â”‚   â””â”€â”€ package.json     # Node.js dependencies
â”œâ”€â”€ tests/               # Integration test scripts
â”‚   â”œâ”€â”€ integration-test.ps1  # Windows PowerShell tests
â”‚   â”œâ”€â”€ integration-test.sh   # Linux/macOS Bash tests
â”‚   â””â”€â”€ README.md        # Test documentation
â”œâ”€â”€ docker-compose.yml   # Multi-service orchestration
â””â”€â”€ README.md           # Project documentation
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License. See LICENSE file for details.

## ğŸ™ Acknowledgments

- Hugging Face Transformers library
- Sentence-BERT for semantic analysis
- FastAPI framework
- React and Vite communities

## ğŸ“ Support

For questions or issues:
1. Check the GitHub Issues page
2. Review the API documentation at `/docs`
3. Ensure all prerequisites are installed
4. Verify Docker services are running

---

**Origo** - Bringing transparency to AI text detection through comprehensive analysis and clear visualization.