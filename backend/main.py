"""
Origo Backend - FastAPI Application
Main application for text analysis and AI detection
"""

import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
import uvicorn

from analysis.scoring import score_fusion
from utils.model_loader import model_loader

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TextAnalysisRequest(BaseModel):
    """Request model for text analysis"""
    text: str = Field(..., min_length=10, max_length=50000, description="Text to analyze (10-50000 characters)")
    
    class Config:
        schema_extra = {
            "example": {
                "text": "This is a sample text that will be analyzed for AI-generated content detection. The analysis will provide various metrics and scores to help determine the likelihood that this text was generated by artificial intelligence."
            }
        }

class TextAnalysisResponse(BaseModel):
    """Response model for text analysis results"""
    overall_score: float = Field(..., description="Overall AI detection score (0-1, higher = more likely AI)")
    global_scores: Dict[str, float] = Field(..., description="Individual analysis component scores")
    paragraphs: list = Field(..., description="Paragraph-level analysis with sentences and words")
    word_analysis: Dict[str, Any] = Field(..., description="Word-level impact analysis")
    analysis_metadata: Optional[Dict[str, Any]] = Field(default=None, description="Analysis metadata and statistics")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager - preload models on startup"""
    logger.info("Starting Origo application...")
    
    # Preload models in background to speed up first request
    try:
        logger.info("Preloading AI models...")
        await asyncio.get_event_loop().run_in_executor(None, model_loader.preload_all_models)
        logger.info("Models preloaded successfully")
    except Exception as e:
        logger.error(f"Error preloading models: {e}")
        logger.info("Models will be loaded on first request")
    
    yield
    
    logger.info("Shutting down Origo application...")

# Create FastAPI application
app = FastAPI(
    title="Origo - AI Text Detection API",
    description="Breaking down the signals of writing origin. Analyzes text to detect AI-generated content using multiple heuristics.",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Configure CORS for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173", "http://127.0.0.1:3000", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """Root endpoint with API information"""
    return {
        "message": "Welcome to Origo - AI Text Detection API",
        "description": "Breaking down the signals of writing origin",
        "version": "1.0.0",
        "endpoints": {
            "analyze": "/analyze - POST: Analyze text for AI detection",
            "health": "/health - GET: Health check",
            "docs": "/docs - Interactive API documentation"
        },
        "disclaimer": "AI-generated text detection is imperfect. This tool provides probabilistic signals â€” not conclusive proof. It is meant to support human judgment, not replace it."
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Quick model availability check
        device = model_loader.get_device()
        return {
            "status": "healthy",
            "device": device,
            "models_loaded": bool(model_loader._models),
            "timestamp": "2024-01-01T00:00:00Z"
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": "2024-01-01T00:00:00Z"
        }

@app.post("/analyze", response_model=TextAnalysisResponse)
async def analyze_text(request: TextAnalysisRequest):
    """
    Analyze text for AI-generated content detection
    
    This endpoint performs comprehensive analysis using multiple heuristics:
    - Perplexity analysis using GPT-2
    - Burstiness analysis (sentence variation patterns)
    - N-gram similarity and repetition detection
    - Semantic coherence using Sentence-BERT
    
    Returns detailed scores and breakdowns at paragraph, sentence, and word levels.
    """
    try:
        logger.info(f"Analyzing text of length: {len(request.text)} characters")
        
        # Validate text length
        if len(request.text.strip()) < 10:
            raise HTTPException(
                status_code=400,
                detail="Text must be at least 10 characters long"
            )
        
        if len(request.text) > 50000:
            raise HTTPException(
                status_code=400,
                detail="Text must be less than 50,000 characters"
            )
        
        # Perform analysis
        result = await asyncio.get_event_loop().run_in_executor(
            None, 
            score_fusion.analyze_text_comprehensive,
            request.text
        )
        
        logger.info(f"Analysis complete. Overall score: {result.get('overall_score', 'N/A')}")
        
        return TextAnalysisResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during text analysis: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error during analysis: {str(e)}"
        )

@app.get("/weights")
async def get_analysis_weights():
    """Get current analysis component weights"""
    return {
        "weights": score_fusion.weights,
        "description": "Weights used for combining different analysis components"
    }

@app.post("/weights")
async def update_analysis_weights(weights: Dict[str, float]):
    """Update analysis component weights (for experimentation)"""
    try:
        score_fusion.update_weights(weights)
        return {
            "message": "Weights updated successfully",
            "new_weights": score_fusion.weights
        }
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"Error updating weights: {str(e)}"
        )

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Global exception handler"""
    logger.error(f"Unhandled exception: {exc}")
    return {
        "error": "Internal server error",
        "message": "An unexpected error occurred",
        "status_code": 500
    }

if __name__ == "__main__":
    # Run the application
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )