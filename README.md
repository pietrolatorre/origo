# Origo - Breaking down the signals of writing origin

Origo is a comprehensive web application that analyzes English text to estimate the probability that it was generated by artificial intelligence. The application provides explainable AI detection through multiple analysis heuristics with detailed breakdowns at word, sentence and paragraph levels.

## ‚ö†Ô∏è Ethical Disclaimer

**Important:** AI-generated text detection is imperfect. This tool provides probabilistic signals ‚Äî not conclusive proof. It is meant to support human judgment, not replace it.

## üéØ Key Features

### Core Analysis
- **4 Analysis Dimensions**: Perplexity, Burstiness, Semantic Coherence, N-gram Similarity
- **Selective Analysis**: Toggle individual dimensions on/off for customized analysis
- **Enhanced Scoring**: Focuses on worst-case patterns using average of values above threshold
- **Performance Optimized**: Parallel processing with intelligent caching (30,000x+ speedup)

### User Interface
- **Language Support**: English (active), Italian (coming soon)
- **Interactive Dimensions**: Click any metric for detailed insights with evidence
- **Smart Display**: Shows top 10 results regardless of threshold to avoid empty displays
- **Processing Time**: Real-time display of analysis duration
- **Floating Paste Button**: Convenient clipboard integration

### Analysis Insights
- **Perplexity Modal**: Top 10 sentences with highlighted impactful parts
- **Burstiness Modal**: Sentence clusters with similar structures and lengths
- **Semantic Modal**: Coherence patterns, topic clusters, and semantic evidence
- **N-gram Modal**: Enhanced pattern detection with exponential frequency weighting

## üìä Scoring Methodology

### New Aggregation Approach
Origo now uses **worst-case aggregation** instead of simple averaging:
- Focuses on text sections scoring above 60% (yellow threshold)
- Calculates average of high-scoring segments only
- Highlights problematic patterns rather than diluting with normal text
- Provides more accurate detection of mixed human/AI content

### Threshold Interpretation
- **Green (0-60%)**: Low AI probability
- **Yellow (60-70%)**: Medium AI probability  
- **Red (70-100%)**: High AI probability

### Dimension Weighting
All dimensions have equal weight (25% each) in the final score when enabled.

## üõ† Technology Stack

### Backend
- **Framework**: FastAPI (Python)
- **AI/ML**: Transformers, PyTorch, Sentence-BERT
- **NLP**: NLTK, spaCy, scikit-learn
- **Models**: GPT-2 (perplexity), all-MiniLM-L6-v2 (semantic analysis)

### Frontend
- **Framework**: React 18 with TypeScript
- **Build Tool**: Vite
- **UI**: Custom CSS with responsive design
- **HTTP Client**: Axios
- **Icons**: Lucide React

### Deployment
- **Containerization**: Docker & Docker Compose
- **Web Server**: Nginx (frontend proxy)
- **Cross-platform**: Windows, macOS, Linux support

## üìã Prerequisites

- **Node.js** 18+ (for frontend development)
- **Python** 3.11+ (for backend development)
- **Docker & Docker Compose** (for deployment)
- **Git** (for version control)

## üöÄ Quick Start with Docker

The fastest way to run Origo is using Docker Compose:

```bash
# Clone the repository
git clone <repository-url>
cd origo

# Start the application
docker-compose up --build

# Access the application
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
# API Documentation: http://localhost:8000/docs
```

## üíª Development Setup

### Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"

# Start development server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Setup

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev

# Build for production
npm run build
```

## üìä Backend Output Structure

### Complete Analysis Response
```json
{
  "overall_score": 0.72,
  "global_scores": {
    "perplexity": 0.68,
    "burstiness": 0.75,
    "semantic_coherence": 0.69,
    "ngram_similarity": 0.78
  },
  "enhanced_analysis": {
    "perplexity_details": {
      "overall_score": 0.68,
      "detailed_sentences": [
        {
          "text": "Sample sentence text",
          "score": 0.82,
          "impactful_parts": [
            {
              "text": "leverage",
              "impact_type": "suspicious_verb",
              "score": 0.85,
              "explanation": "Verb commonly overused in AI-generated text"
            }
          ]
        }
      ]
    },
    "burstiness_details": {
      "overall_score": 0.75,
      "sentence_clusters": {
        "clusters": [
          {
            "structure_signature": "determiner-adjective-noun-verb",
            "sentence_count": 5,
            "uniformity_score": 0.89
          }
        ]
      }
    },
    "semantic_details": {
      "overall_score": 0.69,
      "detailed_evidence": {
        "coherence_patterns": [
          {
            "type": "topic_cluster",
            "coherence_score": 0.91,
            "evidence": "Unusual semantic clustering detected"
          }
        ]
      }
    },
    "ngram_details": {
      "overall_score": 0.78,
      "ngram_analysis": {
        "bigrams": {
          "score": 0.72,
          "details": [
            {
              "text": "it is",
              "frequency": 12,
              "score": 0.85,
              "frequency_ratio": 0.08
            }
          ]
        }
      }
    }
  },
  "paragraphs": [
    {
      "text": "Full paragraph text...",
      "score": 0.74,
      "sentences": [
        {
          "text": "Individual sentence text...",
          "score": 0.81,
          "words": [
            {"word": "furthermore", "score": 0.89}
          ]
        }
      ]
    }
  ],
  "word_analysis": {
    "unique_words": [
      {
        "word": "delve",
        "average_score": 0.94,
        "count": 2,
        "category": "suspicious_verb"
      }
    ],
    "suspicious_words_found": 8,
    "total_unique_words": 145
  },
  "analysis_metadata": {
    "text_length": 1247,
    "word_count": 198,
    "sentence_count": 12,
    "paragraph_count": 3,
    "processing_time_seconds": 2.34,
    "parallel_processing_enabled": true,
    "caching_enabled": true
  }
}
```

## üìñ API Documentation

### Analyze Text Endpoint

**POST** `/analyze`

Analyzes text for AI-generated content detection with optional dimension selection.

#### Request Body
```json
{
  "text": "Your text to analyze here...",
  "enabled_dimensions": {
    "perplexity": true,
    "burstiness": true,
    "semantic_coherence": true,
    "ngram_similarity": false
  }
}
```

#### Parameters
- **text** (required): Text to analyze (10-50,000 characters)
- **enabled_dimensions** (optional): Object specifying which analysis dimensions to include
  - **perplexity**: GPT-2 based predictability analysis
  - **burstiness**: Sentence structure variation analysis
  - **semantic_coherence**: Meaning consistency analysis
  - **ngram_similarity**: Repetitive pattern detection

#### Response
Returns complete analysis results (see Backend Output Structure above).

### Other Endpoints

- **GET** `/` - API information
- **GET** `/health` - Health check
- **GET** `/docs` - Interactive API documentation

## üîß Backend Architecture & Classes

### Analysis Module Classes

#### 1. PerplexityAnalyzer (`backend/analysis/perplexity.py`)
**Status**: ‚úÖ **Enhanced & Production Ready** (Updated with Advanced Detection)

**Purpose**: Multi-dimensional perplexity analysis combining GPT-2 language model scoring with stylistic pattern detection and register authenticity analysis.

**Enhanced Components**:
- **Base Perplexity** (40%): Core GPT-2 perplexity calculation
- **Stylistic Patterns** (30%): AI vocabulary and phrasal pattern detection
- **Register Authenticity** (30%): Formality consistency and naturalness analysis

**Key Methods**:
- `analyze_text(text: str) -> Dict`: **NEW** - Comprehensive analysis combining all components
- `calculate_perplexity(text: str) -> float`: Core perplexity calculation with normalization
- `analyze_stylistic_patterns(text: str) -> Dict`: **NEW** - Detects AI-specific vocabulary and phrases
- `analyze_register_authenticity(text: str) -> Dict`: **NEW** - Analyzes formality and naturalness markers
- `analyze_sentences(sentences: List[str]) -> List[Dict]`: Sentence-level analysis with word breakdown
- `analyze_paragraphs(text: str) -> List[Dict]`: Paragraph-level analysis with sentence breakdown
- `get_word_impact_analysis(text: str) -> Dict`: Word-level impact analysis for highlighting

**Stylistic Pattern Detection**:
- **Suspicious Vocabulary**: Detects AI-preferred verbs ("delve", "leverage", "underscore")
- **Formulaic Phrases**: Identifies mechanical transitions ("it's worth noting that", "in essence")
- **Punctuation Patterns**: Analyzes Oxford comma overuse, em dash abuse, mechanical formatting
- **Transition Constructs**: Detects artificial structuring ("firstly...secondly...finally")

**Register Authenticity Analysis**:
- **Formality Inconsistencies**: Detects mixed casual/academic registers
- **Emotional Variance**: Analyzes unnatural emotional consistency
- **Naturalness Patterns**: Identifies artificial vs. natural language constructions
- **Discourse Markers**: Detects mechanical vs. natural transitions

**Configuration Files**:
- `data/perplexity/llm_red_flags.json`: Suspicious vocabulary patterns
- `data/perplexity/punctuation_patterns.json`: Punctuation analysis rules
- `data/perplexity/register_authenticity.json`: Formality and naturalness markers

**AI Detection Logic**: Enhanced multi-dimensional scoring where base perplexity, stylistic patterns, and register authenticity are weighted together. Higher combined scores indicate stronger AI probability.

**Development Notes**: 
- ‚úÖ **Enhanced with advanced pattern recognition**
- ‚úÖ **Configuration-driven detection rules**
- ‚úÖ **Weighted component scoring system**
- ‚úÖ **Comprehensive linguistic analysis**
- Uses transformer models for high accuracy
- Implements proper tokenization and device management
- Includes comprehensive error handling
- Memory efficient with model caching

#### 2. BurstinessAnalyzer (`backend/analysis/burstiness.py`)
**Status**: ‚úÖ **Enhanced & Production Ready** (Updated with Structural Analysis)

**Purpose**: Multi-dimensional burstiness analysis combining sentence variation patterns with structural consistency detection.

**Enhanced Components**:
- **Base Burstiness** (60%): Traditional sentence length and complexity variation
- **Structural Consistency** (40%): Detects artificial document structure patterns

**Key Methods**:
- `analyze_text(text: str) -> Dict`: **ENHANCED** - Comprehensive analysis combining base and structural components
- `calculate_sentence_length_variation(sentences: List[str]) -> float`: Analyzes sentence length diversity
- `calculate_syntactic_complexity_variation(sentences: List[str]) -> float`: Examines structural complexity patterns
- `calculate_sentence_start_variation(sentences: List[str]) -> float`: Detects repetitive sentence openings
- `calculate_punctuation_patterns(text: str) -> float`: Analyzes punctuation usage diversity
- `analyze_structural_consistency(text: str) -> Dict`: **NEW** - Detects artificial document structure

**Structural Consistency Detection**:
- **Paragraph Uniformity**: Detects unnaturally uniform paragraph lengths
- **Sentence Symmetry**: Identifies mechanical sentence structure patterns
- **Section Balance**: Analyzes artificial intro/body/conclusion ratios
- **Mechanical Patterns**: Detects overuse of transition phrases and formulaic structures
- **Rhythmic Patterns**: Identifies artificially consistent sentence rhythm
- **Formatting Consistency**: Detects unnaturally perfect punctuation and formatting

**Configuration Files**:
- `data/burstiness/structural_patterns.json`: Structural uniformity detection rules

**AI Detection Logic**: Enhanced scoring where base burstiness metrics are combined with structural consistency analysis. Uniform patterns in both dimensions indicate higher AI probability.

**Development Notes**:
- ‚úÖ **Enhanced with structural pattern detection**
- ‚úÖ **Configuration-driven uniformity analysis**
- ‚úÖ **Weighted component scoring system**
- ‚úÖ **Advanced linguistic metrics**
- Implements multiple linguistic metrics
- Weighted scoring system for different pattern types
- Statistical analysis of text variation
- No external dependencies required

#### 3. NgramAnalyzer (`backend/analysis/ngram.py`) 
**Status**: ‚úÖ **Enhanced & Production Ready** (Updated with Advanced N-gram Analysis)

**Purpose**: Advanced n-gram frequency analysis with separate scoring for different n-gram lengths and sophisticated pattern detection.

**Enhanced Components**:
- **Separate N-gram Analysis**: Individual 2-gram, 3-gram, and 4-gram analysis with different thresholds
- **Frequency-Based Scoring**: Threshold-based detection (10% for 2-grams, 8% for 3-grams, 5% for 4-grams)
- **Weighted Scoring**: Different weights based on n-gram length (2-grams: 20%, 3-grams: 40%, 4-grams: 60%)
- **Pattern Analysis**: Enhanced formatting abuse and paragraph structure detection

**Key Methods**:
- `analyze_text_with_patterns(text: str) -> Dict`: **NEW** - Comprehensive analysis with pattern detection
- `calculate_ngram_frequency_score(text: str, n: int, threshold: float) -> Tuple`: **NEW** - Advanced frequency analysis
- `detect_formatting_abuse(text: str) -> Dict`: **NEW** - Detects excessive quotes, bold/italic, icons, reader questions
- `detect_paragraph_patterns(text: str) -> Dict`: **NEW** - Analyzes paragraph structure and uniformity
- `analyze_text(text: str) -> Dict`: **ENHANCED** - Now returns detailed n-gram breakdown
- `extract_ngrams(text: str, n: int) -> List[Tuple]`: Core n-gram extraction
- `calculate_phrase_repetition(text: str) -> float`: Detects repetitive phrases
- `calculate_transition_predictability(text: str) -> float`: Analyzes word transition patterns
- `calculate_lexical_diversity(text: str) -> float`: Measures vocabulary diversity

**Enhanced N-gram Structure**:
```json
{
  "ngram_analysis": {
    "bigrams": {
      "score": 0.45,
      "details": [
        {
          "text": "the process",
          "frequency": 15,
          "score": 0.8,
          "frequency_ratio": 0.12
        }
      ]
    },
    "trigrams": { /* similar structure */ },
    "fourgrams": { /* similar structure */ }
  },
  "pattern_analysis": {
    "formatting": {
      "overall_score": 0.3,
      "component_scores": {
        "quote_abuse": 0.2,
        "formatting_abuse": 0.1,
        "icon_abuse": 0.0,
        "reader_question_abuse": 0.4
      }
    },
    "paragraphs": {
      "overall_score": 0.15,
      "details": {
        "paragraph_count": 8,
        "avg_length": 45.2,
        "length_variance": 12.4
      }
    }
  }
}
```

**Pattern Detection Features**:
- **Formatting Abuse**: Excessive quotes, bold/italic text, emoji overuse
- **Reader Questions**: Detection of "Do you...", "Can you...", "Have you..." patterns
- **Paragraph Analysis**: Line-break detection, length uniformity, repetitive starts
- **Frequency Thresholding**: Dynamic thresholds based on text length and n-gram size

**AI Detection Logic**: Enhanced multi-dimensional scoring where longer n-grams receive higher weights due to increased suspicion. Pattern analysis detects formatting and structural artifacts common in AI text.

**Development Notes**:
- ‚úÖ **Enhanced with separate n-gram length analysis**
- ‚úÖ **Advanced pattern detection capabilities**
- ‚úÖ **Frequency-based thresholding system**
- ‚úÖ **Weighted scoring by n-gram significance**

#### 4. SemanticAnalyzer (`backend/analysis/semantic.py`)
**Status**: ‚úÖ **Fully Implemented & Production Ready**

**Purpose**: Uses Sentence-BERT embeddings to analyze semantic coherence and flow patterns.

**Key Methods**:
- `get_sentence_embeddings(sentences: List[str]) -> np.ndarray`: Generates sentence embeddings
- `calculate_semantic_coherence(sentences: List[str]) -> float`: Measures overall semantic consistency
- `calculate_semantic_flow(sentences: List[str]) -> float`: Analyzes flow between consecutive sentences
- `calculate_topic_consistency(text: str) -> float`: Measures topic consistency across paragraphs
- `calculate_semantic_repetition(sentences: List[str]) -> float`: Detects semantic redundancy
- `analyze_text(text: str) -> Dict`: Comprehensive semantic analysis

**AI Detection Logic**: Extreme semantic coherence or unusual flow patterns may indicate AI generation. Uses cosine similarity between embeddings.

**Development Notes**:
- Uses Sentence-BERT (all-MiniLM-L6-v2) model
- Implements similarity matrix calculations
- Sklearn integration for similarity metrics
- Sophisticated pattern recognition algorithms

#### 5. ScoreFusion (`backend/analysis/scoring.py`)
**Status**: ‚úÖ **Enhanced & Production Ready** (Updated with Parallel Processing)

**Purpose**: Combines multiple analysis heuristics using parallel processing and configurable arithmetic mean weighting for optimal AI detection accuracy and performance.

**Enhanced Features**:
- **Parallel Processing**: All analysis modules run concurrently using ThreadPoolExecutor for 3-4x speed improvement
- **Configuration-Driven Weights**: Loads scoring weights from `data/weights_config.json`
- **Simple Arithmetic Mean**: Equal weighting approach (25% each component)
- **Enhanced Analyzer Integration**: Works with extended perplexity, burstiness, and n-gram analyzers
- **Feature Flag Support**: Configurable enable/disable of enhanced features
- **Timeout Handling**: 30-second timeout per analyzer with graceful fallback

**Key Methods**:
- `analyze_text_comprehensive(text: str) -> Dict`: **ENHANCED** - Main analysis orchestrator with parallel processing
- `_run_parallel_analysis(text: str) -> Dict`: **NEW** - Parallel execution of all analyzers
- `validate_weights()`: Ensures scoring weights sum to 1.0
- `_analyze_paragraphs(text: str) -> List[Dict]`: **ENHANCED** - Uses extended analyzers
- `_analyze_sentences(sentences: List[str]) -> List[Dict]`: **ENHANCED** - Uses extended analyzers
- `_analyze_words(text: str) -> Dict`: Word-level impact analysis
- `_extract_score(result: Any, analysis_type: str) -> float`: **NEW** - Handles both enhanced and legacy formats

**Parallel Processing Architecture**:
```python
with ThreadPoolExecutor(max_workers=4) as executor:
    # Submit all analysis tasks concurrently
    tasks = {
        'perplexity': executor.submit(perplexity_analyzer.analyze_text, text),
        'burstiness': executor.submit(burstiness_analyzer.analyze_text, text),
        'ngram': executor.submit(ngram_analyzer.analyze_text_with_patterns, text),
        'semantic': executor.submit(semantic_analyzer.analyze_text, text)
    }
    # Collect results with timeout handling
    for future in as_completed(tasks):
        result = future.result(timeout=30)
```

**Scoring Weights** (Enhanced Configuration):
- **Perplexity**: 25% (equal weight with enhanced sub-components)
- **Burstiness**: 25% (equal weight with enhanced sub-components)
- **N-gram Similarity**: 25% (equal weight with advanced pattern detection)
- **Semantic Coherence**: 25% (equal weight)

**Enhanced Response Structure**:
```json
{
  "overall_score": 0.78,
  "global_scores": { /* ... */ },
  "enhanced_analysis": {
    "perplexity_details": {
      "overall_score": 0.83,
      "base_perplexity": 0.80,
      "stylistic_patterns": { /* detailed breakdown */ },
      "register_authenticity": { /* detailed breakdown */ }
    },
    "burstiness_details": {
      "overall_score": 0.72,
      "base_burstiness": 0.70,
      "structural_consistency": { /* detailed breakdown */ }
    },
    "ngram_details": {
      "overall_score": 0.65,
      "ngram_analysis": {
        "bigrams": { /* frequency analysis */ },
        "trigrams": { /* frequency analysis */ },
        "fourgrams": { /* frequency analysis */ }
      },
      "pattern_analysis": { /* formatting and structure */ }
    }
  },
  "analysis_metadata": {
    "parallel_processing_enabled": true,
    "enhanced_features_enabled": {
      "stylistic_analysis": true,
      "register_analysis": true,
      "structural_analysis": true
    }
  }
}
```

**Performance Improvements**:
- **Speed**: 3-4x faster analysis through parallel processing
- **Reliability**: Individual analyzer timeouts prevent hanging
- **Scalability**: Concurrent execution scales with available CPU cores
- **Fault Tolerance**: Graceful degradation if individual analyzers fail

**Configuration Files**:
- `data/weights_config.json`: Scoring weights and feature flags

**Development Notes**:
- ‚úÖ **Enhanced with parallel processing architecture**
- ‚úÖ **Supports both enhanced and legacy analyzer formats**
- ‚úÖ **Feature flag system for controlled rollout**
- ‚úÖ **Detailed enhanced analysis reporting**
- ‚úÖ **Comprehensive timeout and error handling**

### Utility Classes

#### ModelLoader (`backend/utils/model_loader.py`)
**Status**: ‚úÖ **Production Ready Singleton**

**Purpose**: Efficiently manages AI model loading with singleton pattern to prevent memory waste.

**Key Methods**:
- `get_gpt2_model() -> tuple`: Loads GPT-2 model and tokenizer
- `get_sentence_transformer() -> SentenceTransformer`: Loads Sentence-BERT model
- `get_device() -> str`: Determines optimal device (CPU/GPU)
- `preload_all_models()`: Startup optimization method

**Models Managed**:
- **GPT-2**: For perplexity calculations
- **all-MiniLM-L6-v2**: For semantic analysis
- **Device Management**: Automatic CPU/GPU detection

#### SentenceSplitter (`backend/utils/sentence_splitter.py`)
**Status**: ‚úÖ **Production Ready**

**Purpose**: Handles text preprocessing, tokenization, and segmentation with NLTK integration.

**Key Methods**:
- `clean_text(text: str) -> str`: Text normalization and cleaning
- `split_into_sentences(text: str) -> List[str]`: NLTK-based sentence segmentation
- `split_into_paragraphs(text: str) -> List[str]`: Paragraph extraction
- `tokenize_words(text: str, remove_stopwords: bool) -> List[str]`: Word tokenization
- `extract_word_positions(text: str, target_words: List[str]) -> Dict`: Position mapping for UI highlighting
- `get_text_statistics(text: str) -> Dict`: Text metrics calculation

### API Integration (`backend/main.py`)
**Status**: ‚úÖ **Production Ready FastAPI Application**

**Key Features**:
- FastAPI framework with automatic OpenAPI documentation
- CORS middleware for frontend integration
- Health check endpoints
- Comprehensive error handling
- Request/response validation with Pydantic models
- Model preloading for optimal performance

## üìä Backend Output Structure

### Main Analysis Response

The `/analyze` endpoint returns a comprehensive JSON structure:

```json
{
  "overall_score": 0.78,                    // Combined AI probability (0-1)
  "global_scores": {                        // Individual component scores
    "perplexity": 0.83,                     // GPT-2 perplexity analysis
    "burstiness": 0.72,                     // Sentence pattern variation
    "semantic_coherence": 0.65,             // Semantic flow consistency
    "ngram_similarity": 0.89                // N-gram repetition patterns
  },
  "paragraphs": [                           // Paragraph-level analysis
    {
      "text": "Paragraph content...",
      "score": 0.80,                        // Paragraph AI probability
      "sentences": [                        // Nested sentence analysis
        {
          "text": "Sentence content...",
          "score": 0.91,                    // Sentence AI probability
          "words": [                        // Word-level contributions
            {
              "word": "thus",
              "score": 0.92                 // Word AI probability
            }
          ]
        }
      ]
    }
  ],
  "word_analysis": {                        // Global word impact analysis
    "unique_words": [
      {
        "word": "hence",
        "average_score": 0.91,               // Average AI probability for word
        "count": 3                          // Frequency in text
      }
    ]
  },
  "analysis_metadata": {                    // Text statistics and analysis info
    "text_length": 1247,                    // Character count
    "word_count": 189,                      // Word count
    "sentence_count": 12,                   // Sentence count
    "paragraph_count": 3,                   // Paragraph count
    "weights_used": {                       // Scoring weights applied
      "perplexity": 0.4,
      "burstiness": 0.2,
      "semantic_coherence": 0.2,
      "ngram_similarity": 0.2
    }
  }
}
```

### Score Interpretation

**Score Ranges**:
- **0.0 - 0.4**: Low AI probability (likely human-written) üü¢
- **0.4 - 0.7**: Medium AI probability (uncertain) üü°  
- **0.7 - 1.0**: High AI probability (likely AI-generated) üî¥

**Component Score Details**:

1. **Perplexity Score** (0-1): 
   - Measures text predictability using GPT-2
   - Higher = more predictable = more AI-like
   - Most reliable single indicator (40% weight)

2. **Burstiness Score** (0-1):
   - Measures sentence structure variation
   - Higher = more uniform = more AI-like
   - Analyzes length, complexity, punctuation patterns

3. **N-gram Similarity Score** (0-1):
   - Measures repetitive patterns and phrases
   - Higher = more repetitive = more AI-like
   - Includes bigram, trigram, and phrase analysis

4. **Semantic Coherence Score** (0-1):
   - Measures semantic flow and consistency
   - Extreme values (very high/low) = more AI-like
   - Uses Sentence-BERT embeddings for analysis

### Hierarchical Analysis Structure

The system provides analysis at multiple granularities:

1. **Document Level**: Overall AI probability and global component scores
2. **Paragraph Level**: Individual paragraph scores with sentence breakdown
3. **Sentence Level**: Individual sentence scores with word contributions
4. **Word Level**: Word-specific AI probability contributions and frequencies

This hierarchical approach enables precise identification of AI-generated content at different text levels, supporting detailed user interface highlighting and explanations.

## üìã Enhanced Configuration System

### Data-Driven Analysis Configuration

Origo now uses a sophisticated configuration system that allows fine-tuning of AI detection parameters without code changes:

**Configuration Files Structure**:
```
backend/data/
‚îú‚îÄ‚îÄ weights_config.json          # Global scoring weights and feature flags
‚îú‚îÄ‚îÄ perplexity/
‚îÇ   ‚îú‚îÄ‚îÄ llm_red_flags.json      # AI-specific vocabulary patterns
‚îÇ   ‚îú‚îÄ‚îÄ punctuation_patterns.json # Suspicious punctuation analysis
‚îÇ   ‚îî‚îÄ‚îÄ register_authenticity.json # Formality and naturalness markers
‚îî‚îÄ‚îÄ burstiness/
    ‚îî‚îÄ‚îÄ structural_patterns.json # Document structure uniformity detection
```

### Key Configuration Features

#### 1. Scoring Weight Configuration (`weights_config.json`)
```json
{
  "scoring_weights": {
    "perplexity": 0.25,        // Equal arithmetic mean weighting
    "burstiness": 0.25,        // As specified in MODIFICHE.md
    "semantic_coherence": 0.25,
    "ngram_similarity": 0.25
  },
  "perplexity_components": {
    "base_perplexity": 0.4,    // Component weighting within perplexity
    "stylistic_patterns": 0.3,
    "register_authenticity": 0.3
  },
  "burstiness_components": {
    "base_burstiness": 0.6,     // Component weighting within burstiness
    "structural_consistency": 0.4
  },
  "feature_flags": {
    "enable_enhanced_perplexity": true,
    "enable_enhanced_burstiness": true,
    "enable_stylistic_analysis": true,
    "enable_register_analysis": true,
    "enable_structural_analysis": true
  }
}
```

#### 2. AI Vocabulary Detection (`llm_red_flags.json`)
- **Suspicious Verbs**: "delve", "leverage", "underscore", "foster", "transcend"
- **Suspicious Modifiers**: "thought-provoking", "nuanced", "multifaceted", "seamless"
- **Formulaic Phrases**: "it's worth noting that", "in essence", "at the end of the day"
- **Transition Constructs**: "firstly...secondly...finally" patterns
- **Weighted Scoring**: Different weights for different pattern types

#### 3. Structural Pattern Detection (`structural_patterns.json`)
- **Paragraph Uniformity**: Variance thresholds for suspicious consistency
- **Sentence Symmetry**: Repetition thresholds for mechanical patterns
- **Mechanical Transitions**: Overuse detection for formulaic phrases
- **Rhythmic Analysis**: Syllable variance thresholds for artificial rhythm

#### 4. Register Authenticity Analysis (`register_authenticity.json`)
- **Formality Mixing**: Detection of casual/academic register inconsistencies
- **Emotional Variance**: Thresholds for unnatural emotional consistency
- **Naturalness Markers**: Artificial vs. natural language construction patterns
- **Discourse Analysis**: Mechanical vs. natural transition patterns

### Configuration Benefits

‚úÖ **Flexible Tuning**: Adjust detection sensitivity without code changes
‚úÖ **Rapid Deployment**: Update detection rules via configuration files
‚úÖ **A/B Testing**: Feature flags enable controlled rollout of enhancements
‚úÖ **Domain Adaptation**: Customize detection for specific text types
‚úÖ **Transparent Rules**: Human-readable detection criteria
‚úÖ **Version Control**: Track detection rule changes over time

## üîß Enhanced Analysis Components

### 1. Enhanced Perplexity Analysis
- **Base Analysis**: Uses GPT-2 language model for predictability measurement
- **Stylistic Patterns**: Detects AI-specific vocabulary and formulaic phrases
- **Register Authenticity**: Analyzes formality consistency and naturalness markers
- **Weight**: 25% (equal weighting as per MODIFICHE.md)

### 2. Enhanced Burstiness Analysis
- **Base Analysis**: Analyzes sentence length and complexity variation
- **Structural Consistency**: Detects artificial document structure patterns
- **Mechanical Detection**: Identifies formulaic transitions and formatting
- **Weight**: 25% (equal weighting)

### 3. Semantic Coherence Analysis
- Uses Sentence-BERT embeddings for semantic flow analysis
- Measures semantic consistency and topic coherence
- Detects extreme coherence patterns suggesting AI generation
- **Weight**: 25% (equal weighting)

### 4. N-gram Similarity Analysis
- Detects repetitive patterns and phrase reuse
- Analyzes n-gram frequencies and lexical diversity
- Identifies mechanical language patterns common in AI text
- **Weight**: 25% (equal weighting)

### Enhanced Detection Capabilities

**Multi-Dimensional Analysis**:
- ‚úÖ **Vocabulary Analysis**: AI-specific word usage patterns
- ‚úÖ **Syntactic Analysis**: Mechanical sentence structure detection
- ‚úÖ **Semantic Analysis**: Flow and coherence pattern recognition
- ‚úÖ **Stylistic Analysis**: Register and formality consistency
- ‚úÖ **Structural Analysis**: Document organization pattern detection
- ‚úÖ **Rhythmic Analysis**: Artificial rhythm and pacing detection

**Configuration-Driven Detection**:
- ‚úÖ **Customizable Thresholds**: Adjust sensitivity per use case
- ‚úÖ **Feature Toggles**: Enable/disable specific detection methods
- ‚úÖ **Weight Adjustment**: Fine-tune component importance
- ‚úÖ **Pattern Updates**: Update detection rules without code changes

## üé® User Interface

### Enhanced Interactive Features
- **Clickable Dimension Analysis**: Click on any metric in the overview to view detailed insights
- **N-gram Analysis Modal**: Dedicated modal with separate tabs for 2-grams, 3-grams, and 4-grams
- **Pattern Detection Display**: Visual representation of formatting abuse and structural patterns
- **Export Functionality**: Generate comprehensive PDF reports with complete analysis data

### Main Features
- **Text Input**: Large textarea with character/word count
- **Analysis Results**: Tabbed interface with multiple views (Overview, Paragraph, Sentence, Word)
- **Score Visualization**: Circular progress indicators with color-coded results
- **Highlighted Text**: Color-coded analysis with tooltips
- **Word Table**: Sortable table with word-level insights
- **Interactive Overview**: Clickable metrics leading to detailed breakdowns

### Enhanced N-gram Analysis Interface
- **Frequency Tables**: Top 10 suspicious n-grams for each length (2, 3, 4-grams)
- **Score Breakdown**: Individual scoring for each n-gram type with color coding
- **Pattern Insights**: Visual indicators for formatting abuse and structural irregularities
- **Detailed Tooltips**: Explanations for each n-gram pattern and its significance

### Color Coding
- üü¢ **Green**: Low AI probability (0-60%)
- üü° **Yellow**: Medium AI probability (60-70%)
- üî¥ **Red**: High AI probability (70-100%)

## üê≥ Docker Deployment

### Production Deployment

```bash
# Build and start services
docker-compose up -d --build

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Update and restart
docker-compose pull
docker-compose up -d --build
```

### Environment Variables

Create `.env` file for custom configuration:

```env
# Backend
PYTHONPATH=/app
PYTHONUNBUFFERED=1

# Frontend
VITE_API_URL=http://localhost:8000
```

## üß™ Testing

### Automated Integration Testing

The project includes comprehensive integration tests in the `tests/` directory:

**Available Test Scripts**:
- `integration-test.ps1` - Windows PowerShell integration tests
- `integration-test.sh` - Linux/macOS Bash integration tests

**What the Tests Cover**:
1. **Docker Configuration**: Validates docker-compose.yml syntax
2. **Service Deployment**: Builds and starts backend and frontend containers
3. **Health Checks**: Verifies backend service is responding
4. **API Testing**: Tests all REST endpoints with sample data including enhanced n-gram analysis
5. **Frontend Validation**: Confirms frontend is accessible
6. **Enhanced Functionality**: Tests parallel processing and new pattern detection
7. **Cleanup**: Properly stops and removes containers

**Running Integration Tests**:
```bash
# Windows PowerShell
cd tests
.\integration-test.ps1

# Linux/macOS Bash
cd tests
chmod +x integration-test.sh
./integration-test.sh
```

### Manual Testing

#### Backend Testing
```bash
cd backend
python -c "from analysis.ngram import ngram_analyzer; print('Enhanced N-gram analyzer working:', ngram_analyzer.analyze_text_with_patterns('Test text'))"
```

#### Frontend Testing
```bash
cd frontend
npm test
```

#### Full System Testing
```bash
# Start services
docker-compose up -d

# Test enhanced API endpoints
curl http://localhost:8000/health
curl -X POST http://localhost:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{"text": "This comprehensive text will leverage various analysis dimensions to provide insights. The text delves into multiple aspects and demonstrates various patterns that might indicate artificial generation."}'

# Test frontend
curl http://localhost:3000

# Stop services
docker-compose down
```

## üìä Performance Considerations

### Enhanced Performance Features
- **Parallel Processing**: 3-4x speed improvement through concurrent analyzer execution
- **Model Caching**: Models loaded once at startup for optimal performance
- **Optimized Thresholds**: Dynamic n-gram thresholds based on text characteristics
- **Efficient Pattern Detection**: Regex-based pattern matching with minimal overhead

### System Requirements
- **Request Timeout**: 2-minute timeout for analysis
- **Text Limits**: 10-50,000 character range
- **Memory Usage**: ~2GB RAM recommended for models
- **Processing Time**: 2-15 seconds with parallel processing (previously 5-30 seconds)
- **CPU Cores**: Better performance with 4+ cores due to parallel execution

### Performance Optimizations
- **ThreadPoolExecutor**: Concurrent execution of all analysis modules
- **Timeout Management**: 30-second timeout per analyzer prevents hanging
- **Graceful Degradation**: System continues if individual analyzers fail
- **Model Preloading**: Startup optimization reduces first-request latency

## üöÄ Deployment Options

### Local Development
- Backend: `uvicorn main:app --reload`
- Frontend: `npm run dev`

### Docker Compose (Recommended)
- Full-stack deployment with one command
- Automatic service orchestration
- Health checks and restart policies

### Cloud Deployment
- **Backend**: Deploy to Heroku, Railway, or AWS
- **Frontend**: Deploy to Vercel, Netlify, or AWS S3
- **Full-stack**: Use Docker on DigitalOcean, AWS ECS, or Google Cloud Run

## üìÅ Project Structure

```
origo/
‚îú‚îÄ‚îÄ backend/              # Python FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ analysis/         # AI detection modules
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Utilities and model management
‚îÇ   ‚îú‚îÄ‚îÄ data/            # üÜï Configuration files for enhanced detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ perplexity/  # Stylistic and register analysis configs
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_red_flags.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ punctuation_patterns.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ register_authenticity.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ burstiness/  # Structural consistency configs
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ structural_patterns.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ weights_config.json  # Global scoring configuration
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # FastAPI app entry point
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt # Python dependencies
‚îú‚îÄ‚îÄ frontend/            # React TypeScript application
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/  # React components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/    # API communication
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types/       # TypeScript definitions
‚îÇ   ‚îî‚îÄ‚îÄ package.json     # Node.js dependencies
‚îú‚îÄ‚îÄ tests/               # Integration test scripts
‚îÇ   ‚îú‚îÄ‚îÄ integration-test.ps1  # Windows PowerShell tests
‚îÇ   ‚îî‚îÄ‚îÄ integration-test.sh   # Linux/macOS Bash tests
‚îú‚îÄ‚îÄ docker-compose.yml   # Multi-service orchestration
‚îî‚îÄ‚îÄ README.md           # Project documentation
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License. See LICENSE file for details.

## üôè Acknowledgments

- Hugging Face Transformers library
- Sentence-BERT for semantic analysis
- FastAPI framework
- React and Vite communities

## üìû Support

For questions or issues:
1. Check the GitHub Issues page
2. Review the API documentation at `/docs`
3. Ensure all prerequisites are installed
4. Verify Docker services are running

---

**Origo** - Bringing transparency to AI text detection through comprehensive analysis and clear visualization.